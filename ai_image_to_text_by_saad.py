# -*- coding: utf-8 -*-
"""AI Image to Text by Saad.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ax5Vv9WeThw6EITx_X1fejObrmXASCJm
"""

#!pip install pytesseract transformers sentencepiece --quiet
!pip install pytesseract
import pytesseract
from PIL import Image,ImageOps,ImageFilter
from google.colab import files
import io
import re
import numpy as np

from transformers import pipeline
rephraser = pipeline("text2text-generation", model="vennify/t5-base-grammar-correction")#Model for grammer correction

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import LinearSVC
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import classification_report
import joblib
import pandas as pd

train_df = pd.read_csv("/content/train.csv")
train_df = train_df[train_df['toxic'] != -1]

X = train_df['comment_text'].astype(str)
y = train_df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')
X_train_vec = vectorizer.fit_transform(X_train)
X_val_vec = vectorizer.transform(X_val)

model = OneVsRestClassifier(LinearSVC())
model.fit(X_train_vec, y_train)

y_pred = model.predict(X_val_vec)
print("\nüìä Classification Report:")
print(classification_report(y_val, y_pred, target_names=y.columns))

joblib.dump(model, 'multi_label_svm_model.pkl')
joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')

import cv2
def blurry(img, threshold=100.0):
    img_cv = np.array(img.convert('L')) #converting the image into grayscale (does't work with the colors)
    laplacian_var = cv2.Laplacian(img_cv, cv2.CV_64F).var() #Using 64 float/ Calculating varience to detect sharp edges
    return laplacian_var < threshold #if varience is less then 100 return

def preprocess(img): #Applying grayscale,Contrast and filter
    img=img.convert('L')
    img=ImageOps.autocontrast(img)
    if blurry(img):
        print("Image is blurry")
        img = img.filter(ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3))
    else:
        print("Image is sharp enough")

    return img

def clean_ocr(text):
    text=text.encode('ascii',errors='ignore').decode()
    text=re.sub(r'[^A-Za-z0-9\s.,!?\'"/]+', ' ', text)#Regex use to avoid dummy values
    text=re.sub(r'\s+',' ',text) #Normalizing spaces
    return text.strip()

def confidence_score(img, confidence_threshold=70):#setting it to 70
    data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)# Returning 2 dictionaires for words and scores
    words = []
    for i in range(len(data['text'])):
        word = data['text'][i].strip()
        try:
          conf = int(data['conf'][i])
        except:
          conf=0
        if word and conf >= confidence_threshold:
            words.append(word)
    return ' '.join(words)

uploaded = files.upload()
print("Image is uploaded\n")

# Allowed image formats
allowed_images = ('.jpg', '.jpeg', '.png')

# Process each uploaded image from bytes into PIL readable
for file_name in uploaded.keys():
    if not file_name.lower().endswith(allowed_images):
        print(f"{file_name} is not an allowed image type")
        continue

    img = Image.open(io.BytesIO(uploaded[file_name]))
    img=preprocess(img)

    extracted_text = confidence_score(img)
    cleaned_text=clean_ocr(extracted_text)

    if cleaned_text.strip():
        enhanced_text = rephraser("fix grammar: " + cleaned_text, max_length=256)[0]['generated_text']
        model = joblib.load('multi_label_svm_model.pkl')
        vectorizer = joblib.load('tfidf_vectorizer.pkl')

        text_vec = vectorizer.transform([enhanced_text])
        prediction = model.predict(text_vec)[0]
        labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
        result = dict(zip(labels, prediction))

        print("\nüìÑ Enhanced Text:\n", enhanced_text)
        print("\nüéØ Predicted Labels:")
        for label, value in result.items():
            print(f"{label}: {'‚úÖ' if value == 1 else '‚ùå'}")
    else:
        print(f"No readable text found in {file_name}")

!pip install flask-ngrok flask transformers pytesseract joblib scikit-learn opencv-python-headless --quiet

#Importing all required liabraries
from flask import Flask, request, render_template_string
from pyngrok import ngrok
from PIL import Image, ImageOps, ImageFilter
import pytesseract
import numpy as np
import cv2
import re
import joblib
from transformers import pipeline

# Load the trained model, TF-IDF vectorizer, and grammar correction pipeline
model = joblib.load('multi_label_svm_model.pkl')
vectorizer = joblib.load('tfidf_vectorizer.pkl')
rephraser = pipeline("text2text-generation", model="vennify/t5-base-grammar-correction")

# Check if the image is blurry using the Laplacian variance method
def blurry(img, threshold=100.0):
    img_cv = np.array(img.convert('L'))
    laplacian_var = cv2.Laplacian(img_cv, cv2.CV_64F).var()
    return laplacian_var < threshold

# Preprocess the image: grayscale, enhance contrast, and sharpen if blurry
def preprocess(img):
    img = img.convert('L')
    img = ImageOps.autocontrast(img)
    if blurry(img):
        img = img.filter(ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3))
    return img

# Extract words from OCR result with a minimum confidence score
def confidence_score(img, confidence_threshold=70):
    data = pytesseract.image_to_data(img, output_type=pytesseract.Output.DICT)
    words = []
    for i in range(len(data['text'])):
        word = data['text'][i].strip()
        try:
            conf = int(data['conf'][i])
        except:
            conf = 0
        if word and conf >= confidence_threshold:
            words.append(word)
    return ' '.join(words)

# Clean OCR text by removing non-ASCII characters and excess whitespace
def clean_ocr(text):
    text = text.encode('ascii', errors='ignore').decode()
    text = re.sub(r'[^A-Za-z0-9\s.,!?\'"/]+', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

html='''
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>AI Text Extractor</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css">
  <style>
    body {
      font-family: 'Segoe UI', sans-serif;
    }

    .container {
      max-width: 800px;
      margin-top: 40px;
    }

    .drop-area {
      border: 2px dashed;
      padding: 30px;
      text-align: center;
      border-radius: 10px;
      cursor: pointer;
      transition: border-color 0.3s;
    }

    .drop-area.dragover {
      border-color: #00d9ff !important;
    }

    .preview-img {
      max-width: 100%;
      max-height: 300px;
      margin-top: 15px;
      border-radius: 8px;
      border: 1px solid #ccc;
    }

    .result-text {
      white-space: pre-wrap;
      padding: 15px;
      border-radius: 8px;
      margin-bottom: 10px;
    }

    .card {
      border-radius: 12px;
    }

    .list-group-item {
      transition: background-color 0.3s, color 0.3s;
    }

    /* Static Progress Bar */
    #loaderWrapper {
      display: none;
      margin-top: 20px;
    }

    #loaderLabel {
      text-align: center;
      font-weight: 500;
      font-size: 0.95rem;
      margin-bottom: 8px;
    }

    #staticProgressBar {
      width: 0%;
      height: 16px;
      border-radius: 10px;
      background: linear-gradient(to right, #00c6ff, #0072ff);
      box-shadow: inset 0 0 4px rgba(0, 0, 0, 0.2);
      transition: width 0.2s ease;
    }

    /* Footer */
    .footer .social-icons a {
      color: #333;
      margin: 0 10px;
      font-size: 1.5rem;
      transition: color 0.3s, transform 0.2s;
      display: inline-block;
    }

    .footer .social-icons a:hover {
      color: #0d6efd;
      transform: scale(1.2);
    }

    .footer .footer-text {
      font-size: 0.95rem;
      color: #555;
    }

    @media (prefers-color-scheme: dark) {
      body {
        background-color: #121212;
        color: #eee;
      }

      .card {
        background-color: #1d1d1d;
        color: #ddd;
      }

      .drop-area {
        background-color: #1e1e1e;
        border-color: #555;
        color: #aaa;
      }

      .result-text {
        background-color: #1e1e1e;
        border: 1px solid #444;
        color: #cfcfcf;
      }

      .list-group-item {
        background-color: #1d1d1d;
        color: #ddd;
        border-color: #444;
      }

      .footer .social-icons a {
        color: #ccc;
      }

      .footer .social-icons a:hover {
        color: #66ccff;
      }

      .footer .footer-text {
        color: #aaa;
      }
    }
  </style>
</head>
<body>
<div class="container">
  <div class="text-center mb-4">
    <h2>AI Text Extractor</h2>
  </div>

  <div class="card p-4 mb-3">
    <form id="uploadForm" method="POST" enctype="multipart/form-data">
      <div class="drop-area" id="drop-area">
        Drag & Drop or Click to Upload Image (.jpg, .jpeg, .png)
        <input type="file" name="file" id="fileInput" accept=".jpg,.jpeg,.png" hidden required>
        <img id="preview" class="preview-img" style="display:none;">
      </div>

      <!-- Static Loader -->
      <div id="loaderWrapper">
        <div id="loaderLabel">Processing...</div>
        <div class="progress">
          <div id="staticProgressBar" class="progress-bar"></div>
        </div>
      </div>

      <div class="text-center mt-3">
        <button class="btn btn-primary" type="submit">Extract Text</button>
      </div>
    </form>
  </div>

  <div id="error" class="alert alert-danger d-none"></div>

  {% if result %}
  <div class="card p-4 mt-4">
    <h4>Enhanced Text:</h4>
    <div class="result-text" id="extractedText">{{ result['text'] }}</div>
    <div class="text-end mt-2">
      <button class="btn btn-secondary btn-sm" onclick="copyText()">Copy</button>
      <button class="btn btn-success btn-sm" onclick="downloadText()">Download</button>
    </div>

    <h4 class="mt-4">Predicted Labels:</h4>
    <ul class="list-group">
      {% for label, value in result['labels'].items() %}
      <li class="list-group-item d-flex justify-content-between align-items-center">
        {{ label.capitalize() }}
        <span class="badge bg-{{ 'success' if value == 1 else 'secondary' }}">
          {{ 'Yes' if value == 1 else 'No' }}
        </span>
      </li>
      {% endfor %}
    </ul>
  </div>
  {% endif %}
</div>

<footer class="footer mt-5 py-3 text-center">
  <div class="social-icons mb-2">
    <a href="https://github.com/itzzSaadi" target="_blank" aria-label="GitHub">
      <i class="fab fa-github"></i>
    </a>
    <a href="https://linkedin.com/in/saad-naseer-b66ba617b" target="_blank" aria-label="LinkedIn">
      <i class="fab fa-linkedin"></i>
    </a>
  </div>
  <div class="footer-text">
    ¬© 2025 Made with ‚ù§Ô∏è by Saad
  </div>
</footer>

<script>
  const dropArea = document.getElementById("drop-area");
  const fileInput = document.getElementById("fileInput");
  const preview = document.getElementById("preview");
  const errorDiv = document.getElementById("error");
  const form = document.getElementById("uploadForm");

  const loaderWrapper = document.getElementById("loaderWrapper");
  const staticProgressBar = document.getElementById("staticProgressBar");
  const loaderLabel = document.getElementById("loaderLabel");

  dropArea.addEventListener("click", () => fileInput.click());
  dropArea.addEventListener("dragover", e => {
    e.preventDefault();
    dropArea.classList.add("dragover");
  });
  dropArea.addEventListener("dragleave", () => dropArea.classList.remove("dragover"));
  dropArea.addEventListener("drop", e => {
    e.preventDefault();
    dropArea.classList.remove("dragover");
    const file = e.dataTransfer.files[0];
    if (file) {
      fileInput.files = e.dataTransfer.files;
      handleFile(file);
    }
  });

  fileInput.addEventListener("change", () => {
    const file = fileInput.files[0];
    handleFile(file);
  });

  function handleFile(file) {
    if (!file.type.startsWith('image/')) {
      showError("Only image files (.jpg, .jpeg, .png) are allowed.");
      fileInput.value = "";
      preview.style.display = "none";
      return;
    }
    errorDiv.classList.add("d-none");
    const reader = new FileReader();
    reader.onload = e => {
      preview.src = e.target.result;
      preview.style.display = "block";
    };
    reader.readAsDataURL(file);
  }

  function showError(msg) {
    errorDiv.classList.remove("d-none");
    errorDiv.innerText = msg;
  }

  form.addEventListener("submit", (e) => {
    e.preventDefault();
    loaderWrapper.style.display = "block";
    let percent = 0;
    const interval = setInterval(() => {
      percent += 2;
      staticProgressBar.style.width = percent + "%";
      if (percent >= 100) {
        clearInterval(interval);
        loaderLabel.innerText = "Complete!";
        setTimeout(() => {
          form.submit(); // finally submit the form
        }, 1000);
      }
    }, 120);
  });

  function copyText() {
    const text = document.getElementById("extractedText").innerText;
    navigator.clipboard.writeText(text);
  }

  function downloadText() {
    const text = document.getElementById("extractedText").innerText;
    const blob = new Blob([text], { type: "text/plain" });
    const link = document.createElement("a");
    link.download = "extracted_text.txt";
    link.href = URL.createObjectURL(blob);
    link.click();
  }
</script>
</body>
</html>
'''

# Initialize Flask application
app = Flask(__name__)

# Define route for homepage that accepts both GET and POST requests
@app.route('/', methods=['GET', 'POST'])
def index():
    result = None

    # Handle form submission
    if request.method == 'POST':
        file = request.files['file']

        # If a file is uploaded, process the image
        if file:
            img = Image.open(file.stream)               # Open uploaded image
            img = preprocess(img)                       # Preprocess the image
            extracted_text = confidence_score(img)      # Extract high-confidence text via OCR
            cleaned_text = clean_ocr(extracted_text)    # Clean the extracted text

            # If text is found, enhance and classify it
            if cleaned_text.strip():
                # Fix grammar using rephraser model
                enhanced_text = rephraser("fix grammar: " + cleaned_text, max_length=256)[0]['generated_text']

                # Transform text into vector and make prediction
                text_vec = vectorizer.transform([enhanced_text])
                prediction = model.predict(text_vec)[0]

                # Define labels and prepare result dictionary
                labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']
                result = {
                    "text": enhanced_text,
                    "labels": dict(zip(labels, prediction))
                }

    # Render the HTML page with the result (if any)
    return render_template_string(html, result=result)

#Auth token to run application on a live server (Shareable)
from pyngrok import conf
conf.get_default().auth_token = "2ywb9uVgAUPlIddOYxgnfSi40D6_3aoC4nNL2WBsJL5dSo5DA"

# Run the app with pyngrok
port = 5000
public_url = ngrok.connect(port)
print("üöÄ Open this URL in a new tab:", public_url)

app.run(port=port)